"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.WorkflowBuilderAgent = void 0;
const messages_1 = require("@langchain/core/messages");
const langgraph_1 = require("@langchain/langgraph");
const n8n_workflow_1 = require("n8n-workflow");
const constants_1 = require("./constants");
const get_node_parameter_tool_1 = require("./tools/get-node-parameter.tool");
const trim_workflow_context_1 = require("./utils/trim-workflow-context");
const workflow_planner_agent_1 = require("./agents/workflow-planner-agent");
const conversation_compact_1 = require("./chains/conversation-compact");
const workflow_name_1 = require("./chains/workflow-name");
const errors_1 = require("./errors");
const add_node_tool_1 = require("./tools/add-node.tool");
const connect_nodes_tool_1 = require("./tools/connect-nodes.tool");
const node_details_tool_1 = require("./tools/node-details.tool");
const node_search_tool_1 = require("./tools/node-search.tool");
const main_agent_prompt_1 = require("./tools/prompts/main-agent.prompt");
const remove_node_tool_1 = require("./tools/remove-node.tool");
const update_node_parameters_tool_1 = require("./tools/update-node-parameters.tool");
const operations_processor_1 = require("./utils/operations-processor");
const stream_processor_1 = require("./utils/stream-processor");
const token_usage_1 = require("./utils/token-usage");
const tool_executor_1 = require("./utils/tool-executor");
const workflow_state_1 = require("./workflow-state");
class WorkflowBuilderAgent {
    checkpointer;
    parsedNodeTypes;
    llmSimpleTask;
    llmComplexTask;
    logger;
    tracer;
    autoCompactThresholdTokens;
    instanceUrl;
    constructor(config) {
        this.parsedNodeTypes = config.parsedNodeTypes;
        this.llmSimpleTask = config.llmSimpleTask;
        this.llmComplexTask = config.llmComplexTask;
        this.logger = config.logger;
        this.checkpointer = config.checkpointer ?? new langgraph_1.MemorySaver();
        this.tracer = config.tracer;
        this.autoCompactThresholdTokens =
            config.autoCompactThresholdTokens ?? constants_1.DEFAULT_AUTO_COMPACT_THRESHOLD_TOKENS;
        this.instanceUrl = config.instanceUrl;
    }
    getBuilderTools() {
        return [
            (0, node_search_tool_1.createNodeSearchTool)(this.parsedNodeTypes),
            (0, node_details_tool_1.createNodeDetailsTool)(this.parsedNodeTypes),
            (0, add_node_tool_1.createAddNodeTool)(this.parsedNodeTypes),
            (0, connect_nodes_tool_1.createConnectNodesTool)(this.parsedNodeTypes, this.logger),
            (0, remove_node_tool_1.createRemoveNodeTool)(this.logger),
            (0, update_node_parameters_tool_1.createUpdateNodeParametersTool)(this.parsedNodeTypes, this.llmComplexTask, this.logger, this.instanceUrl),
            (0, get_node_parameter_tool_1.createGetNodeParameterTool)(),
        ];
    }
    createWorkflow() {
        const builderTools = this.getBuilderTools();
        const tools = builderTools.map((bt) => bt.tool);
        const toolMap = new Map(tools.map((tool) => [tool.name, tool]));
        const callModel = async (state) => {
            if (!this.llmSimpleTask) {
                throw new errors_1.LLMServiceError('LLM not setup');
            }
            if (typeof this.llmSimpleTask.bindTools !== 'function') {
                throw new errors_1.LLMServiceError('LLM does not support tools', {
                    llmModel: this.llmSimpleTask._llmType(),
                });
            }
            const prompt = await main_agent_prompt_1.mainAgentPrompt.invoke({
                ...state,
                workflowJSON: (0, trim_workflow_context_1.trimWorkflowJSON)(state.workflowJSON),
                executionData: state.workflowContext?.executionData ?? {},
                executionSchema: state.workflowContext?.executionSchema ?? [],
                workflowPlan: (0, main_agent_prompt_1.planFormatter)(state.workflowPlan),
                instanceUrl: this.instanceUrl,
            });
            const estimatedTokens = (0, token_usage_1.estimateTokenCountFromMessages)(prompt.messages);
            if (estimatedTokens > constants_1.MAX_INPUT_TOKENS) {
                throw new errors_1.WorkflowStateError('The current conversation and workflow state is too large to process. Try to simplify your workflow by breaking it into smaller parts.');
            }
            const response = await this.llmSimpleTask.bindTools(tools).invoke(prompt);
            return { messages: [response] };
        };
        const shouldAutoCompact = ({ messages }) => {
            const tokenUsage = (0, token_usage_1.extractLastTokenUsage)(messages);
            if (!tokenUsage) {
                this.logger?.debug('No token usage metadata found');
                return false;
            }
            const tokensUsed = tokenUsage.input_tokens + tokenUsage.output_tokens;
            this.logger?.debug('Token usage', {
                inputTokens: tokenUsage.input_tokens,
                outputTokens: tokenUsage.output_tokens,
                totalTokens: tokensUsed,
            });
            return tokensUsed > this.autoCompactThresholdTokens;
        };
        const createPlan = async (state) => {
            const { messages } = state;
            const lastHumanMessage = messages.findLast((m) => m instanceof messages_1.HumanMessage);
            if (typeof lastHumanMessage.content !== 'string') {
                throw new errors_1.ValidationError('Invalid message content for planning');
            }
            const plannerAgent = (0, workflow_planner_agent_1.createWorkflowPlannerAgent)(this.llmSimpleTask, this.parsedNodeTypes);
            const plannerResult = await plannerAgent.plan(lastHumanMessage.content);
            if ('plan' in plannerResult) {
                const { plan, toolMessages } = plannerResult;
                this.logger?.debug('Generated workflow plan: ' + JSON.stringify(plan, null, 2));
                return {
                    workflowPlan: plan,
                    planStatus: 'pending',
                    messages: toolMessages,
                };
            }
            this.logger?.debug('Planner returned text response: ' + plannerResult.text);
            return {
                messages: [
                    new messages_1.AIMessage({
                        content: plannerResult.text,
                    }),
                ],
            };
        };
        const reviewPlan = async (state) => {
            const { workflowPlan } = state;
            if (!workflowPlan) {
                throw new errors_1.ValidationError('No workflow plan to review');
            }
            const userResponse = (0, langgraph_1.interrupt)({
                plan: workflowPlan.plan,
                message: workflowPlan.intro,
            });
            if (userResponse.action === 'approve') {
                return {
                    planStatus: 'approved',
                };
            }
            else if (userResponse.action === 'adjust') {
                return {
                    planStatus: 'rejected',
                    planFeedback: userResponse.feedback ?? 'Please adjust the plan',
                };
            }
            return {};
        };
        const adjustPlan = async (state) => {
            const { messages, planFeedback, workflowPlan } = state;
            const lastHumanMessage = messages.findLast((m) => m instanceof messages_1.HumanMessage);
            if (typeof lastHumanMessage.content !== 'string') {
                throw new errors_1.ValidationError('Invalid message content for plan adjustment');
            }
            const plannerAgent = (0, workflow_planner_agent_1.createWorkflowPlannerAgent)(this.llmSimpleTask, this.parsedNodeTypes);
            const adjustedPlan = await plannerAgent.plan(lastHumanMessage.content, workflowPlan ?? undefined, planFeedback ?? undefined);
            if ('text' in adjustedPlan) {
                return {
                    messages: [
                        new messages_1.AIMessage({
                            content: adjustedPlan.text,
                        }),
                    ],
                };
            }
            const filteredMessages = messages.map((m) => {
                if (m instanceof messages_1.ToolMessage && m.name === 'generate_workflow_plan') {
                    return new messages_1.RemoveMessage({ id: m.id });
                }
                if (m instanceof messages_1.AIMessage && m.tool_calls && m.tool_calls.length > 0) {
                    const hasPlanCall = m.tool_calls.find((tc) => tc.name === 'generate_workflow_plan');
                    if (hasPlanCall) {
                        return new messages_1.RemoveMessage({ id: m.id });
                    }
                }
                return m;
            });
            const planAdjustmentMessage = new messages_1.HumanMessage({ content: planFeedback ?? '' });
            this.logger?.debug('Adjusted workflow plan: ' + JSON.stringify(adjustedPlan, null, 2));
            return {
                workflowPlan: adjustedPlan.plan,
                messages: [...filteredMessages, planAdjustmentMessage, ...adjustedPlan.toolMessages],
                planStatus: 'pending',
                planFeedback: null,
            };
        };
        const shouldModifyState = (state) => {
            const { messages, workflowContext, planStatus } = state;
            const lastHumanMessage = messages.findLast((m) => m instanceof messages_1.HumanMessage);
            if (lastHumanMessage.content === '/compact') {
                return 'compact_messages';
            }
            if (lastHumanMessage.content === '/clear') {
                return 'delete_messages';
            }
            if (workflowContext?.currentWorkflow?.nodes?.length === 0 && messages.length === 1) {
                return 'create_workflow_name';
            }
            if (shouldAutoCompact(state)) {
                return 'auto_compact_messages';
            }
            if (!planStatus && workflowContext?.currentWorkflow?.nodes?.length === 0) {
                return 'create_plan';
            }
            return 'agent';
        };
        const shouldContinue = ({ messages }) => {
            const lastMessage = messages[messages.length - 1];
            if (lastMessage.tool_calls?.length) {
                return 'tools';
            }
            return langgraph_1.END;
        };
        const customToolExecutor = async (state) => {
            return await (0, tool_executor_1.executeToolsInParallel)({ state, toolMap });
        };
        function deleteMessages(state) {
            const messages = state.messages;
            const stateUpdate = {
                workflowOperations: null,
                workflowContext: {},
                messages: messages.map((m) => new messages_1.RemoveMessage({ id: m.id })) ?? [],
                workflowJSON: {
                    nodes: [],
                    connections: {},
                    name: '',
                },
                workflowPlan: null,
                planStatus: null,
                planFeedback: null,
            };
            return stateUpdate;
        }
        const compactSession = async (state) => {
            if (!this.llmSimpleTask) {
                throw new errors_1.LLMServiceError('LLM not setup');
            }
            const { messages, previousSummary } = state;
            const lastHumanMessage = messages[messages.length - 1];
            const isAutoCompact = lastHumanMessage.content !== '/compact';
            this.logger?.debug('Compacting conversation history', {
                isAutoCompact,
            });
            const compactedMessages = await (0, conversation_compact_1.conversationCompactChain)(this.llmSimpleTask, messages, previousSummary);
            return {
                previousSummary: compactedMessages.summaryPlain,
                messages: [
                    ...messages.map((m) => new messages_1.RemoveMessage({ id: m.id })),
                    new messages_1.HumanMessage('Please compress the conversation history'),
                    new messages_1.AIMessage('Successfully compacted conversation history'),
                    ...(isAutoCompact ? [new messages_1.HumanMessage({ content: lastHumanMessage.content })] : []),
                ],
            };
        };
        const createWorkflowName = async (state) => {
            if (!this.llmSimpleTask) {
                throw new errors_1.LLMServiceError('LLM not setup');
            }
            const { workflowJSON, messages } = state;
            if (messages.length === 1 && messages[0] instanceof messages_1.HumanMessage) {
                const initialMessage = messages[0];
                if (typeof initialMessage.content !== 'string') {
                    this.logger?.debug('Initial message content is not a string, skipping workflow name generation');
                    return {};
                }
                this.logger?.debug('Generating workflow name');
                const { name } = await (0, workflow_name_1.workflowNameChain)(this.llmSimpleTask, initialMessage.content);
                return {
                    workflowJSON: {
                        ...workflowJSON,
                        name,
                    },
                };
            }
            return {};
        };
        const workflow = new langgraph_1.StateGraph(workflow_state_1.WorkflowState)
            .addNode('agent', callModel)
            .addNode('tools', customToolExecutor)
            .addNode('process_operations', operations_processor_1.processOperations)
            .addNode('delete_messages', deleteMessages)
            .addNode('compact_messages', compactSession)
            .addNode('auto_compact_messages', compactSession)
            .addNode('create_workflow_name', createWorkflowName)
            .addNode('create_plan', createPlan)
            .addNode('review_plan', reviewPlan)
            .addNode('adjust_plan', adjustPlan)
            .addConditionalEdges('__start__', shouldModifyState)
            .addConditionalEdges('create_plan', (state) => {
            return state.workflowPlan ? 'review_plan' : langgraph_1.END;
        })
            .addConditionalEdges('review_plan', (state) => {
            return state.planStatus === 'approved' ? 'agent' : 'adjust_plan';
        })
            .addEdge('adjust_plan', 'review_plan')
            .addEdge('tools', 'process_operations')
            .addEdge('process_operations', 'agent')
            .addEdge('auto_compact_messages', 'agent')
            .addEdge('create_workflow_name', 'create_plan')
            .addEdge('delete_messages', langgraph_1.END)
            .addEdge('compact_messages', langgraph_1.END)
            .addConditionalEdges('agent', shouldContinue);
        return workflow;
    }
    async getState(workflowId, userId) {
        const workflow = this.createWorkflow();
        const agent = workflow.compile({ checkpointer: this.checkpointer });
        return await agent.getState({
            configurable: { thread_id: `workflow-${workflowId}-user-${userId ?? new Date().getTime()}` },
        });
    }
    static generateThreadId(workflowId, userId) {
        return workflowId
            ? `workflow-${workflowId}-user-${userId ?? new Date().getTime()}`
            : crypto.randomUUID();
    }
    getDefaultWorkflowJSON(payload) {
        return (payload.workflowContext?.currentWorkflow ?? {
            nodes: [],
            connections: {},
        });
    }
    async *chat(payload, userId, abortSignal) {
        this.validateMessageLength(payload.message);
        const { agent, threadConfig, streamConfig } = this.setupAgentAndConfigs(payload, userId, abortSignal);
        try {
            const stream = await this.createAgentStream(payload, streamConfig, agent, threadConfig);
            yield* this.processAgentStream(stream, agent, threadConfig);
        }
        catch (error) {
            this.handleStreamError(error);
        }
    }
    validateMessageLength(message) {
        if (message.length > constants_1.MAX_AI_BUILDER_PROMPT_LENGTH) {
            this.logger?.warn('Message exceeds maximum length', {
                messageLength: message.length,
                maxLength: constants_1.MAX_AI_BUILDER_PROMPT_LENGTH,
            });
            throw new errors_1.ValidationError(`Message exceeds maximum length of ${constants_1.MAX_AI_BUILDER_PROMPT_LENGTH} characters`);
        }
    }
    setupAgentAndConfigs(payload, userId, abortSignal) {
        const agent = this.createWorkflow().compile({ checkpointer: this.checkpointer });
        const workflowId = payload.workflowContext?.currentWorkflow?.id;
        const threadId = WorkflowBuilderAgent.generateThreadId(workflowId, userId);
        const threadConfig = {
            configurable: {
                thread_id: threadId,
            },
        };
        const streamConfig = {
            ...threadConfig,
            streamMode: ['updates', 'custom'],
            recursionLimit: 50,
            signal: abortSignal,
            callbacks: this.tracer ? [this.tracer] : undefined,
        };
        return { agent, threadConfig, streamConfig };
    }
    async createAgentStream(payload, streamConfig, agent, threadConfig) {
        const currentState = await agent.getState(threadConfig);
        const interruptedTask = currentState.tasks.find((task) => task.interrupts && task.interrupts.length > 0);
        if (interruptedTask) {
            const action = payload.message.trim() === constants_1.PLAN_APPROVAL_MESSAGE ? 'approve' : 'adjust';
            const resumeCommand = new langgraph_1.Command({
                resume: {
                    action,
                    feedback: action === 'adjust' ? payload.message : undefined,
                },
            });
            return await agent.stream(resumeCommand, streamConfig);
        }
        return await agent.stream({
            messages: [new messages_1.HumanMessage({ content: payload.message })],
            workflowJSON: this.getDefaultWorkflowJSON(payload),
            workflowOperations: [],
            workflowContext: payload.workflowContext,
        }, streamConfig);
    }
    handleStreamError(error) {
        const invalidRequestErrorMessage = this.getInvalidRequestError(error);
        if (invalidRequestErrorMessage) {
            throw new errors_1.ValidationError(invalidRequestErrorMessage);
        }
        throw error;
    }
    async *processAgentStream(stream, agent, threadConfig) {
        try {
            const streamProcessor = (0, stream_processor_1.createStreamProcessor)(stream);
            for await (const output of streamProcessor) {
                yield output;
            }
        }
        catch (error) {
            await this.handleAgentStreamError(error, agent, threadConfig);
        }
    }
    async handleAgentStreamError(error, agent, threadConfig) {
        if (error &&
            typeof error === 'object' &&
            'message' in error &&
            typeof error.message === 'string' &&
            ['Abort', 'Aborted'].includes(error.message)) {
            const messages = (await agent.getState(threadConfig)).values.messages;
            const abortedAiMessage = new messages_1.AIMessage({
                content: '[Task aborted]',
                id: crypto.randomUUID(),
            });
            await agent.updateState(threadConfig, { messages: [...messages, abortedAiMessage] });
            return;
        }
        if (error instanceof langgraph_1.GraphRecursionError) {
            throw new n8n_workflow_1.ApplicationError('Workflow generation stopped: The AI reached the maximum number of steps while building your workflow. This usually means the workflow design became too complex or got stuck in a loop while trying to create the nodes and connections.');
        }
        throw error;
    }
    getInvalidRequestError(error) {
        if (error instanceof Error &&
            'error' in error &&
            typeof error.error === 'object' &&
            error.error) {
            const innerError = error.error;
            if ('error' in innerError && typeof innerError.error === 'object' && innerError.error) {
                const errorDetails = innerError.error;
                if ('type' in errorDetails &&
                    errorDetails.type === 'invalid_request_error' &&
                    'message' in errorDetails &&
                    typeof errorDetails.message === 'string') {
                    return errorDetails.message;
                }
            }
        }
        return undefined;
    }
    async getSessions(workflowId, userId) {
        const sessions = [];
        if (workflowId) {
            const threadId = WorkflowBuilderAgent.generateThreadId(workflowId, userId);
            const threadConfig = {
                configurable: {
                    thread_id: threadId,
                },
            };
            try {
                const checkpoint = await this.checkpointer.getTuple(threadConfig);
                if (checkpoint?.checkpoint) {
                    const messages = checkpoint.checkpoint.channel_values?.messages ?? [];
                    sessions.push({
                        sessionId: threadId,
                        messages: (0, stream_processor_1.formatMessages)(messages, this.getBuilderTools()),
                        lastUpdated: checkpoint.checkpoint.ts,
                    });
                }
            }
            catch (error) {
                this.logger?.debug('No session found for workflow:', { workflowId, error });
            }
        }
        return { sessions };
    }
}
exports.WorkflowBuilderAgent = WorkflowBuilderAgent;
//# sourceMappingURL=workflow-builder-agent.js.map