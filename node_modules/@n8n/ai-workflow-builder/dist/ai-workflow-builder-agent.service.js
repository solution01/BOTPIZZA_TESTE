"use strict";
var __decorate = (this && this.__decorate) || function (decorators, target, key, desc) {
    var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;
    if (typeof Reflect === "object" && typeof Reflect.decorate === "function") r = Reflect.decorate(decorators, target, key, desc);
    else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;
    return c > 3 && r && Object.defineProperty(target, key, r), r;
};
var __metadata = (this && this.__metadata) || function (k, v) {
    if (typeof Reflect === "object" && typeof Reflect.metadata === "function") return Reflect.metadata(k, v);
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.AiWorkflowBuilderService = void 0;
const tracer_langchain_1 = require("@langchain/core/tracers/tracer_langchain");
const langgraph_1 = require("@langchain/langgraph");
const backend_common_1 = require("@n8n/backend-common");
const di_1 = require("@n8n/di");
const ai_assistant_sdk_1 = require("@n8n_io/ai-assistant-sdk");
const langsmith_1 = require("langsmith");
const errors_1 = require("./errors");
const llm_config_1 = require("./llm-config");
const workflow_builder_agent_1 = require("./workflow-builder-agent");
let AiWorkflowBuilderService = class AiWorkflowBuilderService {
    nodeTypes;
    client;
    logger;
    instanceUrl;
    parsedNodeTypes = [];
    llmSimpleTask;
    llmComplexTask;
    tracingClient;
    checkpointer = new langgraph_1.MemorySaver();
    agent;
    constructor(nodeTypes, client, logger, instanceUrl) {
        this.nodeTypes = nodeTypes;
        this.client = client;
        this.logger = logger;
        this.instanceUrl = instanceUrl;
        this.parsedNodeTypes = this.getNodeTypes();
    }
    async setupModels(user) {
        try {
            if (this.llmSimpleTask && this.llmComplexTask) {
                return;
            }
            if (this.client && user) {
                const authHeaders = await this.client.generateApiProxyCredentials(user);
                const baseUrl = this.client.getApiProxyBaseUrl();
                this.llmSimpleTask = await (0, llm_config_1.gpt41mini)({
                    baseUrl: baseUrl + '/openai',
                    apiKey: '-',
                    headers: {
                        Authorization: authHeaders.apiKey,
                    },
                });
                this.llmComplexTask = await (0, llm_config_1.anthropicClaudeSonnet4)({
                    baseUrl: baseUrl + '/anthropic',
                    apiKey: '-',
                    headers: {
                        Authorization: authHeaders.apiKey,
                        'anthropic-beta': 'prompt-caching-2024-07-31',
                    },
                });
                this.tracingClient = new langsmith_1.Client({
                    apiKey: '-',
                    apiUrl: baseUrl + '/langsmith',
                    autoBatchTracing: false,
                    traceBatchConcurrency: 1,
                    fetchOptions: {
                        headers: {
                            Authorization: authHeaders.apiKey,
                        },
                    },
                });
                return;
            }
            this.llmSimpleTask = await (0, llm_config_1.gpt41mini)({
                apiKey: process.env.N8N_AI_OPENAI_API_KEY ?? '',
            });
            this.llmComplexTask = await (0, llm_config_1.anthropicClaudeSonnet4)({
                apiKey: process.env.N8N_AI_ANTHROPIC_KEY ?? '',
                headers: {
                    'anthropic-beta': 'prompt-caching-2024-07-31',
                },
            });
        }
        catch (error) {
            const llmError = new errors_1.LLMServiceError('Failed to connect to LLM Provider', {
                cause: error,
                tags: {
                    hasClient: !!this.client,
                    hasUser: !!user,
                },
            });
            throw llmError;
        }
    }
    getNodeTypes() {
        const ignoredTypes = [
            '@n8n/n8n-nodes-langchain.toolVectorStore',
            '@n8n/n8n-nodes-langchain.documentGithubLoader',
            '@n8n/n8n-nodes-langchain.code',
        ];
        const nodeTypesKeys = Object.keys(this.nodeTypes.getKnownTypes());
        const nodeTypes = nodeTypesKeys
            .filter((nodeType) => !ignoredTypes.includes(nodeType))
            .map((nodeName) => {
            try {
                return { ...this.nodeTypes.getByNameAndVersion(nodeName).description, name: nodeName };
            }
            catch (error) {
                this.logger?.error('Error getting node type', {
                    nodeName,
                    error: error instanceof Error ? error.message : 'Unknown error',
                });
                return undefined;
            }
        })
            .filter((nodeType) => nodeType !== undefined && nodeType.hidden !== true)
            .map((nodeType, _index, nodeTypes) => {
            const isTool = nodeType.name.endsWith('Tool');
            if (!isTool)
                return nodeType;
            const nonToolNode = nodeTypes.find((nt) => nt.name === nodeType.name.replace('Tool', ''));
            if (!nonToolNode)
                return nodeType;
            return {
                ...nonToolNode,
                ...nodeType,
            };
        });
        return nodeTypes;
    }
    async getAgent(user) {
        if (!this.llmComplexTask || !this.llmSimpleTask) {
            await this.setupModels(user);
        }
        if (!this.llmComplexTask || !this.llmSimpleTask) {
            throw new errors_1.LLMServiceError('Failed to initialize LLM models');
        }
        this.agent ??= new workflow_builder_agent_1.WorkflowBuilderAgent({
            parsedNodeTypes: this.parsedNodeTypes,
            llmSimpleTask: this.llmComplexTask,
            llmComplexTask: this.llmComplexTask,
            logger: this.logger,
            checkpointer: this.checkpointer,
            tracer: this.tracingClient
                ? new tracer_langchain_1.LangChainTracer({ client: this.tracingClient, projectName: 'n8n-workflow-builder' })
                : undefined,
            instanceUrl: this.instanceUrl,
        });
        return this.agent;
    }
    async *chat(payload, user, abortSignal) {
        const agent = await this.getAgent(user);
        for await (const output of agent.chat(payload, user?.id?.toString(), abortSignal)) {
            yield output;
        }
    }
    async getSessions(workflowId, user) {
        const agent = await this.getAgent(user);
        return await agent.getSessions(workflowId, user?.id?.toString());
    }
};
exports.AiWorkflowBuilderService = AiWorkflowBuilderService;
exports.AiWorkflowBuilderService = AiWorkflowBuilderService = __decorate([
    (0, di_1.Service)(),
    __metadata("design:paramtypes", [Object, ai_assistant_sdk_1.AiAssistantClient,
        backend_common_1.Logger, String])
], AiWorkflowBuilderService);
//# sourceMappingURL=ai-workflow-builder-agent.service.js.map